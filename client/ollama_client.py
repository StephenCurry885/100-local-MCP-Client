import asyncio
import os
import sys
from typing import Any

# ---- LlamaIndex (agent + Ollama) ----
try:
    from llama_index.core.agent.workflow import FunctionAgent
    from llama_index.core.workflow import Context
except Exception as e:
    raise SystemExit(
        "æ— æ³•å¯¼å…¥ LlamaIndex Agentï¼Œè¯·å…ˆå®‰è£… llama-indexï¼š\n"
        "  pip install llama-index\n"
        f"åŸå§‹é”™è¯¯: {e}"
    )

try:
    from llama_index.llms.ollama import Ollama
except Exception as e:
    raise SystemExit(
        "æ— æ³•å¯¼å…¥ LlamaIndex Ollama é€‚é…å™¨ï¼š\n"
        "  pip install llama-index-llms-ollama\n"
        f"åŸå§‹é”™è¯¯: {e}"
    )

# MCP å®¢æˆ·ç«¯å·¥å…·å°è£…
try:
    from llama_index.tools.mcp import BasicMCPClient, McpToolSpec
except Exception as e:
    raise SystemExit(
        "æ— æ³•å¯¼å…¥ LlamaIndex çš„ MCP å·¥å…·å°è£…ï¼š\n"
        "  pip install llama-index-tools-mcp\n"
        f"åŸå§‹é”™è¯¯: {e}"
    )

# ---- é…ç½® ----
SERVER_SSE_URL = os.environ.get("MCP_SSE_URL", "http://127.0.0.1:8000/sse")
MODEL_NAME = os.environ.get("OLLAMA_MODEL", "qwen2.5:7b-instruct")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SYSTEM_PROMPT_PATH = os.path.join(BASE_DIR, "system_prompt.txt")

with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as f:
    SYSTEM_PROMPT = f.read().strip()

def build_llm():
    # ç›´æ¥ä½¿ç”¨æœ¬åœ° Ollamaï¼›å¦‚æœä½ ä½¿ç”¨å…¶ä»–æœ¬åœ°æ¨¡å‹ï¼Œæ”¹è¿™é‡Œçš„ model åå­—
    return Ollama(model=MODEL_NAME, request_timeout=120.0)

async def get_agent(mcp_tool: McpToolSpec, llm) -> FunctionAgent:
    tools = await mcp_tool.to_tool_list_async()

    kwargs = dict(
        name="Agent",
        description="agent that interacts with our database via MCP",
        tools=tools,
        llm=llm,
        system_prompt=SYSTEM_PROMPT,
    )

    # ä¹è§‚å°è¯•ï¼šé™åˆ¶æœ€å¤š 3 æ­¥ï¼ˆé¿å…æ— é™å¾ªç¯ï¼‰
    try:
        kwargs["max_steps"] = 3
    except Exception:
        pass

    agent = FunctionAgent(**kwargs)
    print("[Debug] tools bound to agent:", [t.metadata.name for t in tools])

    return agent

async def handle_user_message(message_content: str, agent: FunctionAgent, agent_context: Context, verbose: bool = False) -> str:
    # æŠŠç”¨æˆ·æ¶ˆæ¯äº¤ç»™ä»£ç†ï¼›æ”¯æŒæµå¼äº‹ä»¶ï¼Œæ‰“å°å·¥å…·è°ƒç”¨ä¿¡æ¯
    handler = agent.run(message_content, ctx=agent_context)
    async for event in handler.stream_events():
        if verbose:
            et = type(event).__name__
            if hasattr(event, "tool_name"):
                print(f"[Event] ToolCall -> {getattr(event, 'tool_name', '')}")
            else:
                print(f"[Event] {et}")
    response = await handler
    return str(response)

async def main():
    print(f"ğŸ”— Connecting MCP SSE server: {SERVER_SSE_URL}")
    mcp_client = BasicMCPClient(SERVER_SSE_URL)
    mcp_tool = McpToolSpec(client=mcp_client)

    llm = build_llm()
    agent = await get_agent(mcp_tool, llm)
    context = Context(agent)

    print("ğŸ¤– Agent is ready. Type your message (è¾“å…¥ 'exit' é€€å‡º) ...")
    while True:
        try:
            msg = input("> ").strip()
            # ç›´æ¥è·¯ç”±ï¼ˆé¿å…æ¨¡å‹åšä¸å¿…è¦çš„åˆ¤å®šï¼‰
            if msg in {"è·å–æ•°æ®", "æŸ¥çœ‹å…¨éƒ¨", "æŸ¥è¯¢å…¨éƒ¨"}:
                result = await mcp_tool.client.call_tool("read_data", {"query": "SELECT * FROM people"})
                print("\n[ToolResult/read_data] rows:", result, "\n")
                continue
        except (EOFError, KeyboardInterrupt):
            print("\nBye!")
            break
        if msg.lower() in {"exit", "quit"}:
            break
        if not msg:
            continue
        resp = await handle_user_message(msg, agent, context, verbose=True)
        print("\nAgent:", resp, "\n")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
